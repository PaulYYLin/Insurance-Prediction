{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Health Insurance Cross Sell Prediction\n",
    "\n",
    "### Description: \n",
    "Building a model to predict whether a customer would be interested in Vehicle Insurance is extremely helpful for the company because it can then accordingly plan its communication strategy to reach out to those customers and optimise its business model and revenue.\n",
    "\n",
    "reference : https://www.kaggle.com/datasets/anmolkumar/health-insurance-cross-sell-prediction?select=sample_submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable   |      Definition      |\n",
    "|----------|:-------------|\n",
    "| id |  Unique ID for the customer |\n",
    "| Gender |    Gender of the customer   |\n",
    "| Age | Age of the customer |\n",
    "| Driving_License |   0 : Customer does not have DL, 1 : Customer already has DL  |\n",
    "| Region_Code | Unique code for the region of the customer |\n",
    "| Previously_Insured |   1 : Customer already has Vehicle Insurance, 0 : Customer doesn't have Vehicle Insurance\n",
    "| Vehicle_Age\t | Age of the Vehicle|\n",
    "| Vehicle_Damage |  1 : Customer got his/her vehicle damaged in the past. 0 : Customer didn't get his/her vehicle damaged in the past.  |\n",
    "| Annual_Premium | The amount customer needs to pay as premium in the year |\n",
    "| Policy_Sales_Channel |    Anonymized Code for the channel of outreaching to the customer ie. Different Agents, Over Mail, Over Phone, In Person, etc.   |\n",
    "| Vintage |Number of Days, Customer has been associated with the company |\n",
    "| Response | 1 : Customer is interested, 0 : Customer is not interested |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Dataset/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding\n",
    "\n",
    "## Label Encoder\n",
    "train_data.Gender = LabelEncoder().fit_transform(train_data.Gender)\n",
    "\n",
    "train_data.Vehicle_Damage = LabelEncoder().fit_transform(train_data.Vehicle_Damage)\n",
    "\n",
    "lbe_dic_Veh =dict(zip(train_data.Vehicle_Age.unique(), [2,1,0]))\n",
    "train_data.Vehicle_Age = [lbe_dic_Veh[i] for i in train_data.Vehicle_Age]\n",
    "\n",
    "## Bin counting\n",
    "# Add Region's Features\n",
    "region_code_bcTable  = train_data.groupby('Region_Code').agg({'Vehicle_Damage': np.mean, 'Age' : np.median, 'Gender':np.mean  })\n",
    "region_code_bcTable = region_code_bcTable.rename(columns={'Vehicle_Damage':'Region_Damage_mean','Age': 'Region_Age_median', 'Gender' : 'Female_Ratio' }) # Region's features\n",
    "train_data = train_data.join(region_code_bcTable, on='Region_Code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chose Columns for modeling\n",
    "train_data = train_data[[\n",
    "   'Response',\n",
    "    'id',\n",
    "    'Gender', 'Driving_License','Vehicle_Damage', 'Previously_Insured', # category variables\n",
    "    'Age','Region_Damage_mean','Region_Age_median', 'Female_Ratio','Vehicle_Age',  'Annual_Premium', 'Vintage'\n",
    "    ]]\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_number = len(train_data.query('Response == 0'))/len(train_data.query('Response == 1'))\n",
    "for i in range(round(balance_number*0.5)):\n",
    "    train_data = train_data.append(train_data.query('Response == 1'))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split training & validation data\n",
    "## Avoid training data singnal appear in Validation data\n",
    "dataset = train_data.copy()\n",
    "x_train, x_val, y_train, y_val = train_test_split(dataset.iloc[:, 2:], dataset.iloc[:, 0], test_size=0.3, random_state=4) # Split Traning and Val dataset \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Features\n",
    "selector = SelectKBest(chi2, k=7 ).fit(x_train, y_train)\n",
    "x_train = selector.transform(x_train)\n",
    "x_val = selector.transform(x_val)\n",
    "chosen_features = selector.get_feature_names_out()\n",
    "print('Feature select',list(chosen_features))\n",
    "\n",
    "# Normalization\n",
    "sc = StandardScaler().fit(x_train)\n",
    "x_train = sc.transform(x_train)\n",
    "x_val = sc.transform(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Infra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparing dataset from csv file\n",
    "## Build DataLoader for preprocessed Dataset\n",
    "class TDataset(Dataset):\n",
    "\n",
    "    def __init__(self,x,y):\n",
    "\n",
    "        # Change lsit to Tensor\n",
    "        self.x = torch.from_numpy(np.array(x).astype(np.float32))\n",
    "        self.y = torch.from_numpy(np.array(y).astype(np.float32))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build model framework\n",
    "class LogisticRegression_model(nn.Module):\n",
    "        \n",
    "    def __init__(self,n_features ,hyper_param):\n",
    "        super(LogisticRegression_model, self).__init__()\n",
    "\n",
    "        # Define Layers\n",
    "        self.neurals_1 = hyper_param['neurals_1']\n",
    "        self.linear_0 = nn.Linear(n_features, hyper_param['neurals_1'])\n",
    "        self.act0 = nn.Tanh()\n",
    "        self.linear_1 = nn.Linear(hyper_param['neurals_1'], 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act0(self.linear_0(x))\n",
    "        y_hat = self.sigmoid(self.linear_1(x))\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, dataset=dataset):\n",
    "    global x_train, x_val, y_train, y_val \n",
    "    \n",
    "    n_samples, n_features = x_train.shape # For Model to get Tensot shape to build neural network\n",
    "    print(f'Samples number: {n_samples}, Features number :{n_features}')\n",
    "\n",
    "    # Model Hyperparameters\n",
    "    hyper_param = {\n",
    "    'batch_size': trial.suggest_int('batch_size', 128, 512 , 64),\n",
    "    'epochs' :trial.suggest_int('epochs', 5,15,1),\n",
    "    'lr' : trial.suggest_float('lr',0.05, 0.3) ,\n",
    "    'neurals_1' : trial.suggest_int('neurals_1', 4, n_features )\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "    train_dataset = TDataset(x=x_train, y=y_train)\n",
    "    val_dataset = TDataset(x=x_val, y=y_val)\n",
    "    train_dataloader = DataLoader(dataset = train_dataset, batch_size=hyper_param['batch_size'], shuffle=False)\n",
    "    val_dataloader = DataLoader(dataset = val_dataset, batch_size=hyper_param['batch_size'], shuffle=False)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    model = LogisticRegression_model(n_features,hyper_param)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyper_param['lr'])\n",
    "\n",
    "    for epoch in range(hyper_param['epochs']):\n",
    "        steps = 0\n",
    "        for train_x, train_y in train_dataloader:\n",
    "            steps += 1\n",
    "            y_hat = model(train_x)\n",
    "            loss = criterion(y_hat, train_y.reshape_as(y_hat))\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            print(f'steps: {steps}', end='\\r')\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f'epoch {epoch + 1}: loss = {loss:.8f}')\n",
    "\n",
    "        #writer.add_scalar('Train/Loss', loss.item(), epoch) # For model visulization on tensorboard\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        #Validation part\n",
    "        val_hat = model(val_dataset.x)\n",
    "        auc = roc_auc_score(val_dataset.y, val_hat.detach().numpy())\n",
    "\n",
    "        return auc # Define Onjection function target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "study.optimize(objective, n_trials = 3)\n",
    "df = pd.DataFrame(study.trials_dataframe())\n",
    "df.to_excel('Hyperparameter_trial.xlsx')\n",
    "# Showing optimization results\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial parameters:', study.best_trial.params)\n",
    "print('Best score:', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visulize hyperparameters optimizaing process\n",
    "from optuna.visualization import plot_optimization_history\n",
    "\n",
    "plotly_config = {\"staticPlot\": True}\n",
    "\n",
    "fig = plot_optimization_history(study)\n",
    "fig.show(config=plotly_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualize hyperparameters important weight\n",
    "from optuna.visualization import plot_param_importances\n",
    "\n",
    "fig = plot_param_importances(study)\n",
    "fig.show(config=plotly_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = x_train.shape # For Model to get Tensot shape to build neural network\n",
    "print(f'Samples number: {n_samples}, Features number :{n_features}')\n",
    "\n",
    "model = LogisticRegression_model(n_features = n_features,hyper_param = study.best_trial.params)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=study.best_trial.params['lr'])\n",
    "\n",
    "train_dataset = TDataset(x=x_train, y=y_train)\n",
    "val_dataset = TDataset(x=x_val, y=y_val)\n",
    "train_dataloader = DataLoader(dataset = train_dataset, batch_size=study.best_trial.params['batch_size'], shuffle=False)\n",
    "val_dataloader = DataLoader(dataset = val_dataset, batch_size=study.best_trial.params['batch_size'], shuffle=False)\n",
    "\n",
    "\n",
    "for epoch in range(study.best_trial.params['epochs']):\n",
    "    for train_x, train_y in train_dataloader:\n",
    "        y_hat = model(train_x)\n",
    "        loss = criterion(y_hat, train_y.reshape_as(y_hat))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'epoch {epoch + 1}: loss = {loss:.8f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data \n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(val_dataset.x)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc =  accuracy_score(y_predicted_cls, val_dataset.y.round())\n",
    "    test_loss = criterion( val_dataset.y.reshape_as(y_predicted_cls), y_predicted_cls)\n",
    "\n",
    "    auc = roc_auc_score(val_dataset.y,y_predicted)\n",
    "    print(f'accuracy = {acc: .4f}, auc = {auc: .4f}')\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve( np.array(val_dataset.y), np.array(y_predicted))\n",
    "\n",
    "    plt.plot(fpr,tpr,label=f\"AUC={auc: .4f}\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
